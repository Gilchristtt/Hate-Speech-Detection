# -*- coding: utf-8 -*-
"""Hate Speech Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16p0YLFMpje1awRdy7uZeyixvdvMgdrVv

Importing the necessary libraries for the project.
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install scikeras

import pandas as pd
import re #The regex library for preprocessing stage of the model.

# Libraries for tokenization.
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize

#library for stop words removal.
nltk.download('stopwords')
from nltk.corpus import stopwords

#Libraries for stemmming
from nltk.stem import PorterStemmer

#library for building a vocabulary.
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#library for splitting of dataset into training,validation and testing.
from sklearn.model_selection import train_test_split

#library for building a model.
from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop, Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from keras.regularizers import l1_l2

#library for data visualisation
import matplotlib.pyplot as plt
from wordcloud import WordCloud

#library for BERT model.
!pip install datasets
!pip install transformers
!pip install transformers[torch]
!pip install accelerate -U

#library for T-Test Text Analysis
from scipy import stats

#library for optimisation
!pip install keras
!pip install tensorflow
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV









df = pd.read_csv("/content/drive/MyDrive/HS/hate speech.csv")

df.head(20)

"""Based on the first view of the dataset, I realise there are so many stop words, exclamation marks and other punctuations in the tweet column.As per my research,these must be removed prior to my model training. This will be done during the preprocessing stage of this project. For now, I wanted to probe further to find out the number of rows and columns in the dataset."""

#Displaying the number of columns in the dataset.
print(df.columns)

#Displaying the number of records in the dataset
print(len(df))

"""I discovered that the dataset has 7 columns and 24,783 records. For the purpose of this project,I will need only the hate_speech column and the tweet column in order to predict hate speech. Therefore, I decided to observe the values each column takes.
According to the dataset,0 means that the associated comment is not hate speech and 1 means that it is hate speech.
"""

LABEL = df['hate_speech']
COMMENT = df['tweet']

print(LABEL)
print(COMMENT)

"""Prior to preprocessing,I looked into the normal convention in literature. Based on my research,there are certain objectives I need to achieve.


*   All texts need to be changed to lowercase since python is case sensitive.
*   All url,stop words,numbers need to be removed from the tweet columns. This is because they are not needed in the building of the model.

*   Tokenization needs to be done to break down sentences in the tweet column into smaller units i.e. words.

*   Stemming needs to be used to cut off the ends of words to obtain the base words.

Source:
https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9


"""

#Before preprocessing,I would like to create a custom dataset which has the tweet column and the hate speech score.
new_df= pd.DataFrame()
new_df['TWEETS']=COMMENT
new_df['HATE_SPEECH_SCORE']= LABEL
new_df.head()

"""**CONVERT WORDS TO LOWERCASE**

The first step of my preprocessing is to convert all characters into lowercase since Python is case-sensitive,ython is case sensitive and this can impact my model.
"""

new_df = new_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)
new_df.head()

"""
**REMOVE ANY URL**

The next step is to remove any url in the dataset."""

# Define a regex pattern to match URLs
url_pattern = re.compile(r'https?://\S+')

# Define a function to remove URLs from text
def remove_urls(text):
    return url_pattern.sub('', text)

# Apply the function to the 'text' column and create a new column 'clean_text'
new_df['TWEETS'] = new_df['TWEETS'].apply(remove_urls)

new_df.head(50)

"""**REMOVING NON-WORDS AND WHITESPACES**

This step is to remove any word that does not have any meaning as well as whitespaces for tokenization.
"""

new_df = new_df.replace(to_replace=r'[^\w\s]', value='', regex=True)
new_df['TWEETS'] = new_df['TWEETS'].apply(lambda x: re.sub(r'\bRT\b|\brt\b', '', x, flags=re.IGNORECASE))

new_df.head(51)

"""**REMOVAL OF NUMBERS & PUNCTUATIONS**

This step is to remove all numbers in the tweets since numbers have no relevant meaning as to whether a tweet is a hate speech or not


"""

new_df = new_df.replace(to_replace=r'\d', value='', regex=True)
new_df.head(51)

"""**TOKENIZATION & REMOVAL OF STOP WORDS**

The next stage is to tokenize each sentence in the TWEETS column.
This step is where sentences are broken down into smaller units i.e. words in order to feed the model with specific words.

In addition I removed stop words,which are part of speech that do not convey any meaningful meaning such as are,is etc.
"""

new_df['TWEETS'] = new_df['TWEETS'].apply(word_tokenize)

stop_words = set(stopwords.words('english'))
new_df['TWEETS'] = new_df['TWEETS'].apply(lambda x: [word for word in x if word not in stop_words])
new_df.head(51)

"""**STEMMING**

The main idea of stemming is the cutting the ends of words in order to obtain the base words. for instance (automate,automatic,automation -> automat)
Upon further research ,I noticed there are different types of stemming approaches i.e. Porter stemming, Lovins stemming,Snowball stemming and Lancaster stemming.

For the purpose of this project,I decided to use porter stemming approach. This is because it is simple and fast in its execution,however it produces the best output.

 Snowball stemming is an advanced form of porter stemming but it is computationally expensive hence I don't have the computational resources to use it.
 For Lancaster stemming it is more agressive and less accurate,however the algorithm gets confused when dealing with small words and it is not efficient as well.


 In the case of Lovins stemming,it is fast and remove irregular plurals but it is time consuming and it frequently fails when executed.

Source:https://www.geeksforgeeks.org/introduction-to-stemming/

"""

# Initialize the Porter Stemmer
stemmer = PorterStemmer()

# Define a function to perform stemming on the 'TWEETS' column
def stem_words(words):
    return [stemmer.stem(word) for word in words]

# Define a function to perform stemming on the 'TWEETS' column
def stem_words(words):
    return [stemmer.stem(word) for word in words]

# Apply the function to the 'TWEETS' column.
new_df['TWEETS'] = new_df['TWEETS'].apply(stem_words)

new_df.head(51)

"""**WORD VOCABULARY**

This step is when we create a bag of words such that the most popular words are highlighted.
"""

max_words = 500

# Step 2: Create a Tokenizer instance with num_words set to 50,000
max_words = 500
tokenizer = Tokenizer(num_words=max_words)

tokenizer.fit_on_texts(new_df.TWEETS)

sequences = tokenizer.texts_to_sequences(new_df.TWEETS)
sequences_matrix = pad_sequences(sequences, max_words)

sequences_matrix.shape

"""**DATA VISUALISATION**"""

fig, axs = plt.subplots(1, 2, figsize=(16, 8))

text_pos = " ".join(map(str, new_df.loc[new_df['HATE_SPEECH_SCORE'] == 0, 'TWEETS']))
text_neg = " ".join(map(str, new_df.loc[new_df['HATE_SPEECH_SCORE'] == 1, 'TWEETS']))

train_cloud_pos = WordCloud(collocations=False, background_color='white').generate(text_pos)
train_cloud_neg = WordCloud(collocations=False, background_color='black').generate(text_neg)

axs[0].imshow(train_cloud_pos, interpolation='bilinear')
axs[0].axis('off')
axs[0].set_title('Non-Hate Comments')

axs[1].imshow(train_cloud_neg, interpolation='bilinear')
axs[1].axis('off')
axs[1].set_title('Hate Comments')

plt.show()

"""Upon my research I realised that hate speech detection datasets are subjective however a tweet/comment is classified a hate speech or not based on the creator of the dataset. This explains why similar words can be found boldened in both the Non-hate comment bag and the hate comment bag.

**T-TEST FOR TEXT LENGTHS**

This test helps determine if there's a significant difference in the mean text lengths between hate speech and non-hate speech comments.
"""

# Join the tokenized words back into strings for each tweet
new_df['TWEETS'] = new_df['TWEETS'].apply(lambda x: ' '.join(x))

# Calculate text lengths for hate speech and non-hate speech comments
new_df['text_length'] = new_df['TWEETS'].apply(lambda x: len(x.split()))

# Extract text lengths for hate speech and non-hate speech
hate_speech_lengths = new_df[new_df['HATE_SPEECH_SCORE'] == 1]['text_length']
non_hate_speech_lengths = new_df[new_df['HATE_SPEECH_SCORE'] == 0]['text_length']

# Perform t-test for text lengths
t_stat, p_value = stats.ttest_ind(hate_speech_lengths, non_hate_speech_lengths)
print(f"T-Test for Text Lengths - T-statistic: {t_stat}, p-value: {p_value}")

"""The T-test performed on the text lengths of hate speech and non-hate speech comments yielded a significant result. The obtained T-statistic value of approximately 6.09 and the very small p-value, around \(1.16 \times 10^{-9}\), indicate a substantial difference in the average text lengths between these two categories of comments.
In essence, this statistical analysis confirms that there is a significant discrepancy in text lengths between hate speech and non-hate speech comments within your dataset.

**SPLITTING DATASET INTO TRAINING,VALIDATION AND TESTING**
"""

Xtrain, Xtest, ytrain, ytest = train_test_split(sequences_matrix, new_df.HATE_SPEECH_SCORE, test_size= 0.2, random_state=42)

"""**BUILDING A MODEL USING LSTM**

This code defines a function `create_model` that creates a text classification neural network using LSTM layers. It allows for customization of architecture and regularization, with parameters for input, embedding, LSTM, dense layers, output layer, and model compilation. This customizable model allows for control over architectural elements and regularization techniques.


"""

def create_model(lstm_units=64, dense_units=256, l1_reg=0.01, l2_reg=0.01, learning_rate=0.001):
    inputs = Input(name='inputs', shape=[max_words])
    layer = Embedding(2000, 100, input_length=max_words)(inputs)
    layer = LSTM(lstm_units)(layer)
    layer = Dense(dense_units, name='FC1', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.5)(layer)
    layer = Dense(1, name='out_layer')(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=layer)
    model.compile(loss='binary_crossentropy',
                  optimizer=Adam(learning_rate=learning_rate),
                  metrics=['accuracy'])
    return model

# Create model
    model = create_model()

    history = model.fit(Xtrain, ytrain, validation_split=0.33, epochs = 20, batch_size = 32)

evaluation = model.evaluate(Xtest, ytest)

"""With an epochs of 20 and a batch size of 32,the model accuracy is 77.41%."""

print(history.history.keys())

"""The code defines a model creation function with configurable LSTM and dense layers, then uses GridSearchCV to explore combinations of hyperparameters for the LSTM-based model systematically. It searches for the best hyperparameter values by evaluating the model's performance using cross-validation and displays the best parameter set and its corresponding score achieved during the search.

**EVALUATION & OPTIMISATION FOR LSTM**
"""

# Commented out IPython magic to ensure Python compatibility.
# summarize history for accuracy
# %matplotlib inline
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""This code is to optimize the LSTM model's performance by systematically exploring different hyperparameter combinations and identifying the most effective configuration for the given dataset"""

from keras.models import Model
from keras.layers import Input, LSTM, Dense, Dropout, Activation
from keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV
import numpy as np

# Define the model creation function
def create_model(lstm_units=64, dense_units=128, learning_rate=0.001, dropout_rate=0.5):
    inputs = Input(name='inputs', shape=(10, 50))  # Adjusts the shape to match input data shape
    layer = LSTM(lstm_units)(inputs)  # Use LSTM layer directly on the input data
    layer = Dense(dense_units, name='FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(dropout_rate)(layer)
    layer = Dense(1, name='out_layer')(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=layer)
    model.compile(loss='binary_crossentropy',
                  optimizer=Adam(learning_rate=learning_rate),
                  metrics=['accuracy'])
    return model

# Reshape input data to match the expected shape
Xtrain_reshaped = np.reshape(Xtrain, (-1, 10, 50))


# Define the parameter grid
param_grid = {
    'lstm_units': [32, 64],
    'dense_units': [128, 256],
    'learning_rate': [0.001, 0.01],
    'dropout_rate': [0.3, 0.5],
}

# Creates the KerasClassifier using the correct function name
lstm_model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=32,lstm_units=64, dense_units=128, learning_rate=0.001, dropout_rate=0.5, verbose=1)

# Uses the param_grid in GridSearchCV
grid_search = GridSearchCV(estimator=lstm_model, param_grid=param_grid, cv=3, error_score='raise')

# This code fits the GridSearchCV to the data
grid_result = grid_search.fit(Xtrain_reshaped, ytrain)

# This code displays the best parameters and best score
print("Best parameters:", grid_result.best_params_)
print("Best score:", grid_result.best_score_)

"""After conducting a grid search and found the best parameters,the model achieved an accuracy of 79.81% on its validation set indicating its performance of a particular combination of parameters.

**Training a model using Grid Search**
"""

# Assuming grid_result is your GridSearchCV object with the best_params_ attribute
best_params = grid_result.best_params_

# Creates the best model using the best parameters
best_model = create_model(lstm_units=best_params['lstm_units'],
                          dense_units=best_params['dense_units'],
                          learning_rate=best_params['learning_rate'],
                          dropout_rate=best_params['dropout_rate'])

# Reshapes the test data
Xtest_reshaped = np.reshape(Xtest, (-1, 10, 50))  # Assuming Xtest has the same shape as Xtrain

# Trains the best model on the whole training set
best_model.fit(Xtrain_reshaped, ytrain, epochs=20, batch_size=32, verbose=1)

# Make predictions on the test data
predicted_labels = np.argmax(predictions, axis=1)

# Evaluate the accuracy of the predicted labels against ytest
accuracy = accuracy_score(ytest, predicted_labels)
print("Accuracy:", accuracy)

"""Accuracy score after passing the LSTM model through grid search is 80%.

**Mean Absolute Error**
"""

from sklearn.metrics import mean_absolute_error

# Assuming ytest is your ground truth labels for the test data
mae = mean_absolute_error(ytest, predicted_labels)
print("Mean Absolute Error:", mae)

"""A mean absolute error of approximately 0.2744 suggests the average absolute difference between predicted values and actual values

**Root Mean Squared Error**
"""

from sklearn.metrics import mean_squared_error

# Calculate RMSE
rmse = mean_squared_error(ytest, predicted_labels, squared=False)
print("Root Mean Squared Error:", rmse)

"""The RMSE of 0.6713239247807681 indicates that the model's predictions are closer to the actual values.

**Building a GRU model**

After using LSTM model to train,I decided to use the GRU model to train to see whether I will get a better accuracy score than that of LSTM.
"""

from keras.models import Model
from keras.layers import Input, GRU, Dense, Dropout, Activation
from keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV
import numpy as np

# Define the GRU model creation function
def create_gru_model(units=64, dense_units=128, learning_rate=0.001, dropout_rate=0.5):
    inputs = Input(name='inputs', shape=(10, 50))  # Adjust the shape to match input data shape
    layer = GRU(units=units)(inputs)  # Use GRU layer directly on the input data
    layer = Dense(dense_units, name='FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(dropout_rate)(layer)
    layer = Dense(1, name='out_layer')(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=layer)
    model.compile(loss='binary_crossentropy',
                  optimizer=Adam(learning_rate=learning_rate),
                  metrics=['accuracy'])
    return model

# Reshape your input data to match the expected shape
# Assuming Xtrain is your input data with shape (None, 500)
Xtrain_reshaped = np.reshape(Xtrain, (-1, 10, 50))

# Define the parameter grid
param_grid = {
    'units': [32, 64],
    'dense_units': [128, 256],
    'learning_rate': [0.001, 0.01],
    'dropout_rate': [0.3, 0.5],
}

# Create the KerasClassifier using the correct function name
gru_model = KerasClassifier(build_fn=create_gru_model, epochs=15, batch_size=32, units=64, dense_units=128, learning_rate=0.001, dropout_rate=0.5, verbose=1)

# Use the param_grid in GridSearchCV
grid_search = GridSearchCV(estimator=gru_model, param_grid=param_grid, cv=3, error_score='raise')

# Fit the GridSearchCV to your data
grid_result = grid_search.fit(Xtrain_reshaped, ytrain)

# Display the best parameters and best score
print("Best parameters:", grid_result.best_params_)
print("Best score:", grid_result.best_score_)

"""After training the GRU model,I had a similar accuracy score of 79.80% to the LSTM model.

This code to predictsto predict labels using the best model obtained from the grid search (best_estimator_) on the test data.
"""

best_model = grid_result.best_estimator_
Xtest_reshaped = np.reshape(Xtest, (-1, 10, 50))  # Reshape test data
y_pred = best_model.predict(Xtest_reshaped)  # Make predictions on the test data

accuracy = accuracy_score(ytest, y_pred)
print("Accuracy:", accuracy)

"""**MAE for GRU model**"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(ytest, y_pred)
rmse = np.sqrt(mean_squared_error(ytest, y_pred))

print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)

# Save the best model
import joblib
joblib.dump(model, 'lstm_model.joblib')

"""**EVALUATION & OPTIMISATION FOR GRU MODEL**

****

**BUILDING A MODEL USING BERT**
"""

!pip install ktrain

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/MyDrive/HS/hate speech.csv')
df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)

import ktrain
from ktrain import text

MODEL_NAME = 'bert-base-uncased'
t = text.Transformer(MODEL_NAME, maxlen=48, classes=[0, 1, 2])
trn = t.preprocess_train(df_train.tweet.values, df_train['class'].values)
val = t.preprocess_train(df_test.tweet.values, df_test['class'].values)

model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=8)
learner.fit_onecycle(5e-5, 1)
learner.validate()

new_df = pd.DataFrame()
processed_tweets = ['tweet1', 'tweet2', 'tweet3']
labels = [0, 1, 0]
new_df = pd.DataFrame({'TWEETS': processed_tweets, 'HATE_SPEECH_SCORE': labels})

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

checkpoint = "distilbert-base-cased"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Text input you want to test
text_to_predict = "Peel up peel up bring it back up rewind back where I'm from they move Shaq from the line ooooow who tf said that trash!!?"

# Tokenize the text and prepare it for the model
inputs = tokenizer(text_to_predict, return_tensors="pt")

# Forward pass through the model
outputs = model(**inputs)

# Get the predicted class probabilities using softmax
probs = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]

# Swap predicted class labels (0 for hate speech, 1 for non-hate speech)
predicted_class = 1 - int(torch.argmax(outputs.logits))
print("Predicted Class:", predicted_class)
print("Class Probabilities:", probs)

"""This code demonstrates how to perform text classification using a pre-trained BERT model through the ktrain library. Specifically, it loads the BERT model, preprocesses the input text using the specified settings, and predicts the class label for a given input text sequence."""

from ktrain import text
import ktrain

MODEL_NAME = 'bert-base-uncased'

t = text.Transformer(MODEL_NAME, maxlen=48, classes=[0, 1, 2])

# Assuming you have the trained model loaded
model = t.get_classifier()
predictor = ktrain.get_predictor(model, preproc=t)

# Text input you want to test
text_to_predict = "bitch"

# Predict whether the input text contains hate speech or not
prediction = predictor.predict(text_to_predict)

print("Predicted Class:", prediction)

from google.colab import files

MODEL_PATH = '/path/to/save/BERT_model'  # Replace with the actual path to your saved model

# Download the model file
files.download(MODEL_PATH)

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load BERT model and tokenizer
checkpoint = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Save the model and tokenizer using their save_pretrained() methods
model.save_pretrained('saved_model')  # Replace 'saved_model' with your preferred path
tokenizer.save_pretrained('saved_model')  # Save tokenizer along with the model

# Later, you can load the model and tokenizer using from_pretrained()
loaded_model = AutoModelForSequenceClassification.from_pretrained('saved_model')
loaded_tokenizer = AutoTokenizer.from_pretrained('saved_model')

import os

# Get the current directory
current_directory = os.getcwd()

# List the contents of the current directory
contents = os.listdir(current_directory)
print(contents)